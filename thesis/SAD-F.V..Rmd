---
title: "Statistica e Analisi dei Dati"
author: "Francesco Vicidomini"
output: 
    pdf_document: 
        toc: yes
---

```{r setup, include=FALSE}
library(readxl)
library(knitr)
library(randomcoloR)
library(plyr)
library(TeachingDemos)
source("../scripts/utils.R")
source("../scripts/nonOmo.R")
source("../scripts/boxplot.with.outlier.label.R")
source("../scripts/clustering.R")

#source("https://raw.githubusercontent.com/talgalili/R-code-snippets/master/boxplot.with.outlier.label.r")

knitr::opts_chunk$set(echo = TRUE)
```
<!--numerazione dell'inidice-->
\setcounter{page}{1} 		
\pagenumbering{roman} 
<!--\listoftables-->
\listoffigures
\newpage
<!--numerazione della tesina -->
\setcounter{page}{1} 		
\pagenumbering{arabic} 
# 1. Introduzione
L'obiettivo del progetto è quello di analizzare e confrontare il numero di laureati(delle università pubbliche) fra le varie regioni italiane nell'anno 2016, i dati sono stati presi dal sito _[dati.istat](http://dati.istat.it/)_ in formato xlsx e sono stati importati in R utilizzando la funzione **read_excel**.  

```{r}
ds <- read_excel("../dataset/dataset.xlsx", skip = 1)
```

Una volta importato il dataset esso presenta i segenti valori  
\begin{figure}[h]
\begin{center}
\includegraphics[width=350px]{../img/ds.png}
\caption{Il dataset}
\end{center}
\end{figure}

Per semplificare il lavoro i valori delle colonne sono stati salvati all'interno delle variabili riportate di seguito
```{r warning = FALSE}
#variabili
regioni <- c("Piemonte", "Valle d'Aosta", "Liguria", "Lombardia",
             "T. Alto Adige", "Veneto", "F.V.Giulia", "E. Romagna",
             "Toscana", "Umbria", "Marche", "Lazio", "Abruzzo",
             "Molise", "Compania", "Puglia", "Basilicata",
             "Calabria", "Sicilia", "Sardegna")
row.names(ds) <- regioni
triennale <- ds$`corsi di laurea di I livello`
magistrale <- ds$`corsi di laurea magistrale biennale`
unico <- ds$`corsi di laurea magistrale a ciclo unico`
vecchio <- ds$`corsi di laurea del vecchio ordinamento`
```

## 1.1 Calcolo dei rapporti
I valori risultano molto altalenanti, quindi potrebbero risultare difficili da rappresentare graficamente in futuro, per ovviare a questo problema si è pensato di rappresentare i dati come il rapporto fra i laureati della regione e il numeri di laureati totali per lo specifico ciclo di studi, i risultati verranno poi moltiplicati per 100 e arrotondati alle sole prime due cifre decimali.  
I valori sono stati calcolati utilizzando i seguenti comandi:

```{r}
rapTriennali <- round((triennale/sum(triennale))*100, digits = 2)
rapMagistrali <- round((magistrale/sum(magistrale))*100, digits = 2)
rapUnico <- round((unico/sum(unico))*100, digits = 2)
rapVecchio <- round((vecchio/sum(vecchio))*100, digits = 2)
dsRapporto <- data.frame(
                        "Rapporto triennali"= rapTriennali,
                        "Rapporto magistrali"= rapMagistrali,
                        "Rapporto a ciclo unico"= rapUnico,
                        "Rapporto vecchio ordinamento"= rapVecchio
                        )
row.names(dsRapporto) <- regioni
```

```{r echo=FALSE}
kable(dsRapporto)
```

Possiamo notare che la somma degli elementi su ogni colonna si avvicina di molto a 100, non è esattamento 100 per colpa dell'arrotondamento.

# 2. Grafici a barre
Per una rapida visualizzazione dei dati si è deciso di mostrare i grafici a barre dei laureati nelle varie regioni d'Italia. I grafici verranno mostrati con la funzione scritta ad hoc **showBar**
```{r eval = TRUE, echo=FALSE}
showBar
```
essa prende in input una delle colonne del data frame **dsRapporti**, i nomi delle regioni per definire le label della legenda, il titolo da dare al grafico e il numero di righe del dataframe, i colori delle barre sono stati definiti grazie alla funzione **distinctColorPalette** della libreria **randomcoloR**.

## 2.1 Grafico del rapporto dei laureati triennali
```{r}
rowRapporto <- nrow(dsRapporto)
showBar(rapTriennali, regioni, "Rapporto Triennale", rowRapporto, "topright")
```

Il grafico ci dice che la regione con il numero più alto di laureati triennali è la Lombardia(14.82) mentre la regione con il rapporto più basso è la Valle d'Aosta(0.12).

## 2.2 Grafico del rapporto dei laureati magistrali
```{r echo=FALSE}
showBar(rapMagistrali, regioni, "Rapporto Magistrale", rowRapporto, "topright")
```

Il rapprto più alto viene registrato in ancora in Lombardia(15.08), il rapporto più basso viene registrato dalla Valle d'Aosta con un rapporto di laureati magistrali pari a 0.01.

## 2.3 Grafico del rapporto dei laureati a ciclo unico
```{r echo=FALSE}
showBar(rapUnico, regioni, "Rapporto ciclo unico", rowRapporto,"topleft")
```

La regione con il rapporto più alto è di nuovo la Campania con 11.68 mentre la regione con il rapproto più basso è la Valle d'Aosta con 0.01.

## 2.4 Grafico del rapporto dei laureati con il vecchio ordinamento
```{r echo=FALSE}
showBar(rapVecchio, regioni, "Rapporto vecchio ordinamento", rowRapporto, "topright")
```

La regione con il rapporto più alto è il Lazio 14.70, il rapporto più basso si registra in Umbria avente un rapproto pari a 0.

# 3. Diagramma di Pareto
La legge del Principio di Pareto(Legge dell'80/20) afferma che _"la maggior parte degli effetti dipendono da un numero ristretto di cause"_. Per costruire il diagramma di Pareto occorre classificare i dati, ordinarli, disegnare un grafico a barre, aggiungere la linea cumulativa e aggiungere le informazioni di base.  
Come dati da classificare è stato scelto il totale dei tassi di laureti italiani, di seguito viene mostrata la funzione utilizzata per costruire il diagramma.

```{r}
pareto
```

```{r eval=FALSE, echo=TRUE}
pareto("Diagramma di Pareto di tutti i laureati nel 2016", rowRapporto)
```


\begin{figure}[h]
\includegraphics{../img/pareto2.png}
\caption{Pareto dei laureati italiani}
\end{figure}


Dal grafico si evince che:

- l'12.6% dei laureati italiani è lombardo
- il 78.5% dei laureati italiani è lombardo, laziale, romagnolo, campano, siculo, toscano, piemontese, veneto
- l' 8.9% dei laureati italiani vive nelle restanti uncidi regioni

Possiamo dire che il principio di Pareto non è soddisfatto perchè l'80% dei laureati esce dal più del 50% delle università sparse per le varie regioni(invece che nel 20%).

# 4. Retta di regressione
La relazione fra due variabili quantitative può essere rappresentata graficamente con un diagramma di dispessione(anche detto scatterplot). Per poter costruire questo grafico abbiamo bisogno di individuare le due variabili da mettere in relazione, la coppia di variabili è formata da una variabile **dipendente**(posta sull'asse delle Y) e una **indipendente**(posta sull'asse delle X); ogni coppia di osservazioni viene rappresentata come un punto. Il risultato finale sarà un grafico contenente un insieme di punti attraverso il quale cerchiamo di individuare un trend.  
La correlazione tra le variabili quantitative si ricava tramite la **covarianza** che indica quanto le due variabili variano insieme, ovvero quanto esse siano dipendenti.  
Il **coefficiente di correlazione** ci consente di misurare la "forza" del legame fra le due variabili prese in esame.  
Le variabili prese in esame sono:

- rapporto dei laureati magistrali dipendente
- rapporto dei laureati triennali indipendente

Le due variabili presentano una **covarianza** pari a 
```{r echo=TRUE}
cov(rapTriennali, rapMagistrali)
```
e un **coefficiente di correlazione** pari a 
```{r echo=TRUE}
cor(rapTriennali, rapMagistrali)
```
Possiamo notare che la covarianza è positiva quindi possiamo dire che i valori delle due variabili sono correlati e che tendono a variare insieme, anche il coefficiente di correlazione è positivo e assume un valore molto prossimo ad 1 questo ci da una ulteriore conferma della bontà di questo legame.

## 4.1 Regressione lineare semplice
Il modello di regressione lineare semplice è esprimibile attraverso l'equazione di una retta tale da riuscire a interpolare i punti della nuvola dello scatterplot meglio di tutte le altre rette possibili.  
L'equazione della retta è:
$$Y =  \alpha + \beta X$$
Dove $\alpha$ è l'**intercetta** e $\beta$ è il **coefficiente angolare**. L'intercetta corrisponde all'ordinata del punto di intersezione tra la retta interpolante e l'asse delle ordinate. Il coefficiente angolare indica la pendenza della retta un coefficiente angolare positivo indica una retta crescente, mentre, un coefficiente angolare negativo indica una retta descrescente. 

## 4.2 Costruzione del modello di regressione lineare semplice
```{r}
slr <- lm(rapTriennali~rapMagistrali)
print(slr)
```
Possiamo notare che il coefficiente angolare è positivo quindi avremo una retta crescente.  
Di seguito viene mostrato lo script fatto per mostrare lo scatterplot della retta di regressione, per una migliore visualizzazione i dati riguardanti i tassi triennali e magistrali sono stati scalati utilizzando la funzione **scale()**.
```{r echo=TRUE}
scaleTriennale <- scale(rapTriennali)
scaleMagistrale <- scale(rapMagistrali)
modelloScalato <- lm(scaleTriennale~scaleMagistrale)
plot(scaleTriennale, scaleMagistrale, main= "Retta di regressione", xlab="rapporto Triennale",
ylab = "rapporto Magistrale", col="red")
abline(modelloScalato)
stime <- fitted(modelloScalato)
segments(scaleTriennale, stime, scaleTriennale, scaleMagistrale, col="magenta")
text(x=scaleTriennale, y=scaleMagistrale, label = regioni, cex = 0.6)
```

Dal grafico si evince che il campione è positivamente correlato in quanto abbiamo una retta interpolante crescente e che avendo una covarianza positiva ma non di molto i segmenti non presentano sempre la stessa distanza.

## 4.3 Grafico dei residui
Con l'analisi dei residui effettuiamo una analisi più accurata sul come la retta interpola i dati e di come i residui si dispongono intorno alla retta interpolante influenzandone la posizione. I residui sono posti sulle ordinate mentre e i valori della variabile indipendente sono disposti sull'asse delle ascisse.  
Come prima cosa vengono calcolati i residui del modello con la funzione **resid()** dopodichè viene stampato il grafico dei residui
```{r}
residui<-(resid(slr))
plot(rapMagistrali, residui, main = "Grafico dei residui", xlab = "Rapporto Magistrale",
ylab = "Residui", col= "red")
text(x= rapMagistrali, y=residui+0.05, labels = regioni, cex=0.6)
abline(h=0, col="blue", lty=2)
```

# 5. Statistica descrittiva

## 5.1 Quantile
La funzione **quantile()** è utile per analizzare un campione di dati, essa restituisce cinque valori:

- Q0 valore minimo dei dati
- Q1 primo quantile, questo valore corrisponde al dato per il quale il 25% dei dati risultano alla sinistra del valore e il 75% risulta a destra
- Q2 secondo quantile, questo valore corrisponde al dato per il quale il 50% dei dati risultano alla sinistra del valore e il 50% risulta a destra; questo valore viene anche chiamato **mediana**
- Q3 terzo quantile, questo valore corrisponde al dato per il quale il 75% dei dati risultano alla sinistra del valore e il 25% risulta a destra
- Q4 valore massimo dei dati

## 5.2 Calcolo dei quantili
In questo paragrafo verranno calcolati i quantili per le varie colonne del dataset dei tassi, i valori percentiali corrispondono ai seguenti quantili:

- 0% sta a Q0
- 25% sta a Q1
- 50% sta a Q2
- 75% sta a Q3
- 100% sta a Q4

```{r}
quantile(rapTriennali)
quantile(rapMagistrali)
quantile(rapUnico)
quantile(rapVecchio)
```

## 5.3 Boxplot
Con il boxplot andiamo a rappresentare graficamente quanto è stato calcolato con in quantili. La rappresentazione grafica corrisponde ad una scatola i cui estremi corrispondono a Q1 e Q3, e da una linea centrale che rappresenta Q2, in alto e in basso sono presenti due linee orizzontali dette "baffi".  
Il baffo inferiore corrisponde al valore più piccolo delle osservazioni Q0
$$Q_0 >= Q_1-1.5*(Q_3-Q_1)$$
il baffo superiore corrisponde al valore più alto delle osservazioni(Q4) 
$$Q_4 <= Q_3+1.5*(Q_3-Q_1)$$
Se tutti i dati rientrano nell'intervallo
$$ Q_1-1.5*(Q_3-Q_1) <= x <= Q_3+1.5*(Q_3-Q_1)$$
allora i baffi saranno posti in corrispondenza del valore minimo e del valore massimo del campione, se invece sono presenti valori al di fuori dell'intervallo questi verranno considerati come valori anomomali(**outlier**) i valori anomali vanno comunque analizzati per capire da cosa è stato scaturito quel valore anomalo.
Il boxplot può essere utilizzato anche per esaminare altre caratteristiche della distribuzione quali:

- **centralità** espressa come mediana
- **forma** la forma che può assumere può essere simmatrica o asimmetrica essa può essere dedotta esaminando le distanza tra il primo e il terzo quartile
- **dispersione** possiamo dedurre la dispersione esaminando la distanza che passa da Q1 a Q3  

Per mostrare i boxplot è stata sviluppata la funzione **showBarPlot**

```{r echo=FALSE, eval= FALSE, warning=FALSE}
#script per mettere i boxplot in una sola pagina
par(mfrow=c(2,2))
#la funnzione showBoxPlot
showBoxPlot <- function(label, rapporto, name, title, color, lim){
    boxplot.with.outlier.label(label_name = label, rapporto,
    names=c(name), main=title,
    col= color, pars=list(ylim=c(0,lim)))
}
#chiamate alla funzione
showBoxPlot(regioni, rapTriennali, "Rapporto Triennale",
            "Boxplot triennali", "green", 15)
showBoxPlot(regioni, rapMagistrali, "Rapporto Magistrale",
            "Boxplot magistrali", "yellow", 16)
showBoxPlot(regioni, rapUnico, "Rapporto Ciclo Unico",
            "Boxplot ciclo unico", "blue", 12)
showBoxPlot(regioni, rapVecchio, "Rapporto Vecchio Ordinamento",
            "Boxplot vecchio ordinamento", "purple", 15)
```

La funzione showBar per disegnare i boxplot con le label usa la funziona **boxplot.with.outlier.label** presa da __\href{https://raw.githubusercontent.com/talgalili/R-code-snippets/master/boxplot.with.outlier.label.r}{questo repository}__.  
Showbar prende in input:

* le label che sarebbero le regioni,
* la colonna dei tassi,
* il nome della colonna,
* titolo da dare al boxplot,
* colore del box,
* lunghezza dell'asse y


\begin{figure}[ht]
\begin{center}
\includegraphics{../img/boxplot2.png}
\caption{Boxplot dei laureati}
\end{center}
\end{figure}
\newpage <!--RICORDATELO!!!! -->

## 5.4 Analisi del boxplot
<!--TODO da aggiustare-->
Analizzando il boxplot possiamo arrivare alle seguenti conclusioni:

* i dati risultano avare una maggiore dispersione nella colonna dei laureati triennali
* possiamo notare la compattezza dei dati relativi alle colonne dei laureati a ciclo unico e dei laureati con vecchio ordinamento
* per i laureati magistrali la maggior parte dei dati si trova più in basso rispetto alla mediana

# 6. Analisi dei cluster
L'analisi dei cluster è una metodologia che permette di raggruppare un campio ampio vari sottoinsiemi più piccoli detti cluster. L'obettivo di questa analisi è quello di **raggruppare** fra loro elementi simili.

## 6.1 Costruizione 
Abbiamo detto che raggruppiamo in base alla **somiglianza** degli elementi, per calcolare questa somiglianza applichiamo i metodi delle distanze. Le distanze tra tutte le possibili coppie di unità vengono inserite in una matrice simmetrica _D_ di cardinalità _n x n_. Il metodo che che utilizzero per calcolare le distanze è quello della **metrica Euclidea**.  
Avendo definito come misurare la distanza andiamo a definire come raggruppare le unità osservate, esistono tre metodi per il raggruppamento:

1. Enumerazione completa
2. Metodi gerarchici
3. Metodi non gerarchici

L'**enumerazione completa** è molto onerosa dal punto di vista computazionale, in quanto questo metodo cerca di ottimizzare(minimizzare) una fissata funzione obiettivo per ogni possibile partizione delgi individui all'interno di un cluster.  
I **metodi gerarchici** possono essere di due tipi **agglomerativi** o **divisivi**, i tipi agglomerativi partono da una situazione in cui ci sono _n_ cluster distinti con cardinalità pari a uno, per arrivare a un solo cluster con cardinalità _n_. Invece i metodi divisi fanno l'inverso dei metodi agglomerativi, ovvero, partono da un unico cluster con cardinalità _n_ per arrivare ad _n_ cluster con cardinalità 1.  
L'obiettivo dei metodi gerarchi è arrivare alla costruzione di un **dendrogramma** ovvero il grafico ad albero che rappresenta la sequenza delle partizioni. Il dendrogramma riporta sull'ordinata i livelli di distanza e sulle ascisse i signoli elementi, questa rappresentazione consente di farci capire rapidamente i livelli di distanza fra gli elementi.  
I **metodi non gerarchici** permettono di riallocare gli elementi già classificati in un precedente livello di analisi.

## 6.2 Calcolo delle misure di non omogeneità interna e totale
Le misure di non omogeneità ci servono per calcolare quanto gli elementi siano distinti fra loro, esse vanno calcolate sia all'interno del sigolo cluster sia tra due cluster distinti. Una volta calcolate le misure saremo in grado di dire che gli elementi appartenti allo stesso cluster sono quanto più possibile omogenei fra loro a differenza di due(o più) elementi appartenti a due(o più) cluster distinti.  
Gli individui vanno scelti in modo tale da minimizzare le misure di non omogeneità all'interno dei cluster(within) e da massimizzare la misura di non omogeneità tra gruppi distinti(between).  
Questo calcolo va effettuato sia per i metodi gerarchici sia per i metodi non gerarchici e corrisponde al _rapporto fra la somma dei quadrati degli elementi sotto la diagonale principale della matrice delle distanze e il numero degli individui_

## 6.3 Cluster con metodi non gerarchici: k-means
Il metodo non gerachico k-means richiede che il numero di cluster venga specificato a priori e da in output un'unica partizione. Per garantire la convergenza del metodo iterativo si considera la matrice contenete i quadrati delle distanze euclidee.  
Vantaggi del k-means:  

* rapidità nei calcoli
* gli elementi hanno la possibilità di raggrupparsi o allontanarsi

Fra gli svantaggi c'è il fatto che la classificazione finale può essere influenzata da:

* ordine in cui sono stati presi i vettori
* proprietà geometriche dei vettori delle misure
* scelta iniziale dei _k_ vettori
* possibile convergenza versono un **ottimo locale** ciò implica che considerando un insieme di partenza diverso si più giungere a una diversa partizione finale.

### 6.3.1 Implementazione k-means
Il metodo k-means è stato implementato con la funzione **mykm** che vado a mostrare di seguito
```{r, echo=FALSE}
print(mykm)
```

La funzione prende in input un dataframe(**ds**), il numero di cluster(**k**) e il numero di iterazioni da svolgere(**iter**) sceglie i centroidi come punto di riferimento e rappresenta graficamente i cluster.
Il dataframe dato in input corrisponde alle prime due colonne del dataset dei tassi dei laureati quindi al tasso di laureati triennali e magistrali, il numero di cluster va da 2 a 5, e le iterazioni sono sempre 10.

```{r}
mykm(dsRapporto[1:2], 2, 10)

mykm(dsRapporto[1:2], 3, 10)

mykm(dsRapporto[1:2], 4, 10)

mykm(dsRapporto[1:2], 5, 10)
```

## 6.4 Osservazioni finali
I valori di non omogeneità migliori si ottengo nella partizionamento con k = 4 e k = 5 infatti i loro valori sono rispettivamente 94.4.0% e 95.4%, si è decisio di fermarsi con la divisione in cluster con un k = 5 in quanto già l'output del k-means con 5 cluster ci mostara un alto grado di non omogeneità pari al 95.4% ma possiamo notare(sia con k=4 che con k=5) che ci sono dei cluster molto piccoli composti da uno e due elementi e quindi l'utilità del cluster si va a perdere. Quindi si è deciso di prendere in esame le partizioni con 2 e 3 cluster che hanno come valore di non omogeneità rispettivamente 76.8% e 89.4% le misure ottenute da questi due tipi di partizionamento verranno poi confrontate con le misure ottenute dai metodi gerarchici.

## 6.5 Metodi gerarchici
I metodi gerarchici sono di cinque tipi di:

* singolo
* completo
* medio
* centroide
* mediana

I metodi del centroide e della mediana non calcolano la matrice della distanza partendo dalle distanze precedenti ma dai baricentri dei cluster.
L'applicazione dei metodi gerarchici verrà svolta applicando la distanza euclidea, ogni metodo gerarchico viene calcolato dividendo i dati prima in due e poi in tre cluster in modo tale da poter confrontare i risultati ottenuti con il risultato del metodo non gerarchico k-means.  
Per calcolare i vari tipi di metodi gerarchici è stato realizzato lo script **metodiGerarchici()** che viene mostrato di seguito

```{r echo=FALSE, eval = FALSE}
metodiGerarchici
```

La funzione prende in input il dataset(dsRapporti[1:2]), il numero di cluster, il metodo di agglomerazione da applicare(legame singolo, completo, medio, centroide o mediana) e il titolo da dare al grafico. La funzione calcola la matrice delle distanze con la funzione **dist()**, la funzione **hclust()** crea il dendrogramma prendendo come parametro la matrice delle distanze e il metodo di agglomerazione, infine plotta il dendrogramma.

## 6.5.1 Metodo del legame singolo
Con questo metodo la distanza fra il gruppo $G_1$(contenente $n_1$ individui) e il gruppo $G_2$(contenente $n_2$ individui) viene definita come la minima distanza fra tutte le $n_1n_2$ distanze calcolabili fra ogni individuo di $G_1$ e ogni individuo di $G_2$.  
Inizialmente consideriamo un insieme di cluster, in seguito si cerca nella matrice delle distanze il coefficiente di distanza minima e si raggruppano gli individui nello stasso cluster.  
Se due individui riusultano simili verrà costruito un cluster con questi due elementi se gli elementi simili sono più di due verrà effettuata una scelta casuale e si procederà alla creazione di una nuova matrice. Questo processo verrà ripetuto fin quando sarà possibile aggiungere individui al cluster.   

* **Vantaggi**: risulta più facile individuare gruppi di qualsiasi forma ed è più facile mettere in evidenza valori anomali
* **Svantaggi**: si possono trovare nello stesso cluster individui non simili

```{r echo=FALSE}
cluster2Table <- buildClusterTable(2)
cluster2Table$B.T[1] <- "76.8%"
cluster3Table <- buildClusterTable(3)
cluster3Table$B.T[1] <- "89.4%"

#metodo del legame singolo 2 cluster
metodiGerarchici(dsRapporto[1:2], regioni, 2, "single", "Metodo del legame singolo")
cls <- applyClustering(dsRapporto[1:2], "euclidean", "single", 2)
paste("Rapporto tra omogeneità cluster e omogenetà totale, B/T= ", round(cls$bt*100, digits = 2), "%", sep="")
cluster2Table$B.T[2] <- paste(round(cls$bt*100, digits = 2), "%", sep="")

#metodo del legame singolo 3 cluster
metodiGerarchici(dsRapporto[1:2], regioni, 3, "single", "Metodo del legame singolo")
cls <- applyClustering(dsRapporto[1:2], "euclidean", "single", 3)
paste("Rapporto tra omogeneità cluster e omogenetà totale, B/T= ", round(cls$bt*100, digits = 2), "%", sep="")
cluster3Table$B.T[2] <- paste(round(cls$bt*100, digits = 2), "%", sep="")

```

## 6.5.2 Metodo del legame completo
Questo metodo definisce la distanza fra $G_1$ e $G_2$ come la distanza massima fra tutte le $n_1, n_2$ distanze che si possono calcolare tra ogni individuo di $G_1$ e $G_2$. Il metodo del legame completo privilegia l'omogeneità tra gli elementi del gruppo a scapito della differenziazione fra gruppi infatti il dendrogramma presenterà dei rami più lunghi rispetto a quelli del dendrogramma ottenuto con il metodo del legame singolo

```{r echo=FALSE}
#metodo del legame completo 2 cluster
metodiGerarchici(dsRapporto[1:2], regioni, 2, "complete", "Metodo del legame completo")
cls <- applyClustering(dsRapporto[1:2], "euclidean", "complete", 2)
paste("Rapporto tra omogeneità cluster e omogenetà totale, B/T= ", round(cls$bt*100, digits = 2), "%", sep="")
cluster2Table$B.T[3] <- paste(round(cls$bt*100, digits = 2), "%", sep="")

#metodo del legame completo 3 cluster
metodiGerarchici(dsRapporto[1:2], regioni, 3, "complete", "Metodo del legame completo")
cls <- applyClustering(dsRapporto[1:2], "euclidean", "complete", 3)
paste("Rapporto tra omogeneità cluster e omogenetà totale, B/T= ", round(cls$bt*100, digits = 2), "%", sep="")
cluster3Table$B.T[3] <- paste(round(cls$bt*100, digits = 2), "%", sep="")
```

## 6.5.3 Metodo del legame medio
In questo caso la distanza fra i due gruppi viene definita come la media aritmetica delle distanze fra tutte le coppie che compongono i due gruppi.

```{r echo=FALSE}
#metodo del legame medio 2 cluster
metodiGerarchici(dsRapporto[1:2], regioni, 2, "average", "Metodo del legame medio")
cls <- applyClustering(dsRapporto[1:2], "euclidean", "average", 2)
paste("Rapporto tra omogeneità cluster e omogenetà totale, B/T= ", round(cls$bt*100, digits = 2), "%", sep="")
cluster2Table$B.T[4] <- paste(round(cls$bt*100, digits = 2), "%", sep="")

#metodo del legame medio 3 cluster
metodiGerarchici(dsRapporto[1:2], regioni, 3, "average", "Metodo del legame medio")
cls <- applyClustering(dsRapporto[1:2], "euclidean", "average", 3)
paste("Rapporto tra omogeneità cluster e omogenetà totale, B/T= ", round(cls$bt*100, digits = 2), "%", sep="")
cluster3Table$B.T[4] <- paste(round(cls$bt*100, digits = 2), "%", sep="")
```

## 6.5.4 Metodo del centroide
Con il metodo del centroide la distanza fra i due gruppi viene definita come la distanza fra i centroidi, ovvero, tra le medie campionarie calcolate sugli elementi appartenenti ai due gruppi

```{r echo=FALSE}
#metodo del centroide 2 cluster
metodiGerarchici(dsRapporto[1:2], regioni, 2, "centroid", "Metodo del centroide")
cls <- applyClustering(dsRapporto[1:2], "euclidean", "centroid", 2)
paste("Rapporto tra omogeneità cluster e omogenetà totale, B/T= ", round(cls$bt*100, digits = 2), "%", sep="")
cluster2Table$B.T[5] <- paste(round(cls$bt*100, digits = 2), "%", sep="")

#metodo del centroide 3 cluster
metodiGerarchici(dsRapporto[1:2], regioni, 3, "centroid", "Metodo del centroide")
cls <- applyClustering(dsRapporto[1:2], "euclidean", "centroid", 3)
paste("Rapporto tra omogeneità cluster e omogenetà totale, B/T= ", round(cls$bt*100, digits = 2), "%", sep="")
cluster3Table$B.T[5] <- paste(round(cls$bt*100, digits = 2), "%", sep="")
```

## 6.5.4 Metodo della mediana
Il metodo della mediana è simile a quello del centroide ma in questo caso la procedure è indipendete dal numero di cluster

```{r echo=FALSE}
#metodo del centroide 2 cluster
metodiGerarchici(dsRapporto[1:2], regioni, 2, "median", "Metodo della mediana")
cls <- applyClustering(dsRapporto[1:2], "euclidean", "median", 2)
paste("Rapporto tra omogeneità cluster e omogenetà totale, B/T= ", round(cls$bt*100, digits = 2), "%", sep="")
cluster2Table$B.T[6] <- paste(round(cls$bt*100, digits = 2), "%", sep="")

#metodo del centroide 3 cluster
metodiGerarchici(dsRapporto[1:2], regioni, 3, "median", "Metodo del centroide")
cls <- applyClustering(dsRapporto[1:2], "euclidean", "median", 3)
paste("Rapporto tra omogeneità cluster e omogenetà totale, B/T= ", round(cls$bt*100, digits = 2), "%", sep="")
cluster3Table$B.T[6] <- paste(round(cls$bt*100, digits = 2), "%", sep="")
```

## 6.6 Tabelle riassuntive

```{r echo=FALSE}
kable(cluster2Table)
kable(cluster3Table)
```

Conclusioni:

* La misura di non omogeneità **peggiore** si ottiene con i metodi gerarchici del legame singolo su 2 cluster(28.69%)
* La misura di non omogeneità **migliore** si ottiene con il metodo k-means su 3 cluster 89.4%
* Altre misure di non omogeneità accettabili sono quelle con un valore superiore al 70% quindi oltre al metodo del legame singolo su 2 cluster non possiamo accettare neanche la misura ottunuta con il metodo del legame singolo su 3 cluster.

# 7. Inferenza statistica
Una indagine statisica si effettua su una popolazione che può essere finita o infinita, le caratteristiche della popolazione possono essere ottenute osservando la totalità della popolazione oppure un sottoinsieme di essa(un campione) se ci troviamo di fronte a una popolazione infita possiamo studiare le sue caratteristiche solamente attraverso un suo campione.  
L'inferenza statistica ha il compito di estendere le caratteristiche osservate in un un campione a tutta la popolazione, il problema dell'inferenza statistica è quello di riuscire a studiare la popolazione attraverso un parametro non noto, l'inferenza statistica si basa su due metodi di indagine essi sono **stima dei parametri** e **verifica delle ipotesi**.  
La stima dei parametri ha lo scopo di determinare i valori non noti delle caratteristiche di una popolazione come ad esempio media, mediana, varianza ecc... dai valori delle caratteristiche ricavati possiamo effettuare due tipi di stime, la **stima puntuale** dove stimiamo un paramentro non noto usando un singolo valore reale e la **stima per intervallo** dove si cerca di individuare un limite inferiore e un limite superiore(un intervallo) entro i quali sia compreso il parametro non noto con un certo **grado di fiducia**.  
Per quanto riguarda la verifica delle ipotesi si definisce una ipotesi se un parametro non noto del campione è accettabile.  
Per studiare i problemi dell'inferenza statistica occorre conoscere le variabili aleatorie **discrete**(assume un numero finito di valori) o **continue**(può assumere infiniti valori nell'intevallo preso in considerazione). La variabile scelta per questo studio è la variabile aleatoria normale, essa è una variabile continua.

## 7.1 Variabile aleatoria normale
La normale costituisce una distribzione limite alla quale tendono le variabili aleatoria in opportuni casi.  
Prenderemo in esame la tematica "**chilometri di corsa percorsi da 110 atleti durante i loro allenamenti settimanali**".\footnote{per motivi di spazio il data frame popolazioneAtleti non viene mostrato nella tesina ma può essere visto lanciando il comando \textit{View(popolazioneAtleti)}} Il campione di atleti è stato  generato usando la funzione __rnorm()__. 
```{r echo = TRUE, eval = FALSE }
popolazioneAtleti <- data.frame(km = floor(rnorm(110, 24, 3)))
#salvo i dati in un CSV
write.table(popolazioneAtleti, file = "dataset/sample.csv", row.names = FALSE, sep = ";")
```

Per calcolare la  densità di una variabile aleatoria $X$ è data dalla seguente formula:
$$f_X(x) = \frac{1}{\sigma \sqrt{2\pi }} exp\left \{ - \frac{(x - \mu)^2)}{2 \sigma^2} \right \}, x \epsilon \mathbb{R} (\mu \epsilon \mathbb{R}, \sigma > 0)$$

```{r echo=FALSE, eval=FALSE}
popolazioneAtleti <- read.csv(file="../dataset/sample.csv", header=TRUE, sep=";")
```

la densità di __popolazioneAtleti__ in viene calcolata dal seguente script R

```{r eval=FALSE}
valMedio <- mean(popolazioneAtleti$km)
stdDev <- sd(popolazioneAtleti$km)
print(paste("Valore medio = ", round(valMedio, digits=2),
            ", Deviazione standard =", round(stdDev, digits = 2))
      )

curve(dnorm(x, mean = valMedio, sd = stdDev), 
      from = 13, to = 35,
      main = "Km percorsi dai 110 atleti in una settimana di allenamento"
      )
abline(v=valMedio, col= "red", lwd=2, lty = 2)
```

Per calcolare la **densità** come prima cosa calcoliamo il valore medio e deviazione standard del campione, infine con la funzione __curve()__ che prende come input la densità normale del campione calcolata con la funzione __dnorm(valore medio, deviazione standard)__ e i parametri __from__ e __to__ che indicano l'intervallo del grafico, sul grafico è stata anche riportata la retta che rappresenta il valore medio, si può notare che il valore medio(23.27) coincide con massimo della curva, essendoci una deviazione standar alta(3.04) possiamo dire che al decrescere della deviazione standard la curva si appiattirà.  
Per calcolare la **funzione di distribuzione normale** si usa la funzione __pnorm()__ di R, di seguito viene mostrata il codice per calcolare la funzione di distribuzione normale sul campione.

```{r eval = FALSE}
titolo = paste("km fatti in una settimana di allenamento mu =",
               round(valMedio, digits = 2),
               ", sigma =", round(stdDev, digits = 2)
               )
curve(pnorm(x, mean = valMedio, sd = stdDev), from = 13, to = 35, main = titolo)
abline(v=20, col= "red", lwd= 1, lty = 2)
abline(v=28, col= "red", lwd= 1, lty = 2)
```

Notiamo che la funzione cresce nell'intervallo che va da 20 a 28 per poi arrivare ad 1 intorno al 29 per poi stabilizzarsi.  
Possiamo anche calcolare i quantili del campione preso in analisi grazie alla funzione R __qnorm()__ 

```{r eval = FALSE}
q <- c(0, 0.25, 0.50, 0.75, 1)
qnorm(q, mean = valMedio, sd = stdDev)
```

Conclusioni: 

* **25%** dei km è pari a 21.22 vicino al punto in cui la probabilità comincia a crescere
* **50%** dei km è pari a 23.27 uguale al valore medio
* **75%** dei km è pari a 25.32 

## 7.2 Stima dei parametri non noti
Possiamo stimare i parametri non noti con due metodi, **metodo dei momenti** **metodo della massima verosomiglianza**. Dato che vogliamo stimare i paramentri non noti di una distribuzione normale la scelta
del metodo stimatore è indifferente perchè entrambi i metodi ci porteranno allo stesso risultato.  
Troviamo uno stimatore per il valore medio(media campionaria)$\mu$ e uno per la varianza $\sigma^2$

```{r eval = FALSE}
mu <- valMedio
sigmaQ <- (length(popolazioneAtleti$km)-1) * var(popolazioneAtleti$km)/length(popolazioneAtleti$km)
```

```{r echo=FALSE, eval = FALSE}
print(paste("stimatore valore medio mu =", mu, ", stimatore varianza sigma^2 =", sigmaQ))
```

## 7.3 Intervalli di confidenza
Alla stima puntuale di un parametro non noto di un campione si preferisce un **intervallo di confidenza** entro il quale sarà presente in parametro non noto con un certo **grado di fiducia**.  
In una distribuzione normale si cerca di individuare un intervallo di confidenza da 1 a $\infty$ per:

1. $\mu$ nel caso in cui $\sigma^2$ della popolazione sia nota
2. $\mu$ nel caso in cui $\sigma^2$ della popolazione **non** sia nota
3. $\sigma^2$ nel caso in cui $\mu$ della popolazione sia nota
4. $\sigma^2$ nel caso in cui $\mu$ della popolazione **non** sia nota

Tutti i casi elencati in precedenza sono riassunti nel seguente grafico:

```{r echo=FALSE, eval = FALSE}
curve(dnorm(x,mean=0,sd=1),from=-3, to=3,axes=FALSE,ylim=c(0,0.5), xlab="",ylab="",main="Densità normale standard")
text(0,0.05,expression(1-alpha))
axis(1,c(-3,-1,0,1,3),c("",expression(-z[alpha/2]), 0,expression(z[alpha/2]),""))
vals<-seq(-3,-1,length=100)
x<-c(-3,vals,-1,-3)
y<-c(0,dnorm(vals),0,0)
polygon(x,y,density=20,angle=45)
vals<-seq(1,3,length=100)
x<-c(1,vals,3,1)
y<-c(0,dnorm(vals),0,0)
polygon(x,y,density=20,angle=45)
abline(h=0)
text(-1.5,0.05,expression(alpha/2))
text(-2, 0.20, "Regione rifiuto")
text(1.5,0.05,expression(alpha/2))
text(2, 0.20, "Regione rifiuto")
text(0, 0.30, "Regione accettazione")
```

## 7.3.1 Intervallo per $\mu$ con $\sigma^2$ nota
Utilizziamo il **metodo pivotale** e consideriamo una variabile aleatoria con valore medio nullo e varianza unitaria. La variabile aleatoria standardizzata dipende dal campione scelto e dal parametro non noto $\mu$ per questo può essere interpretata come una variabile aleatoria di pivot. Applicando il metodo pivotale conosciamo il limite inferiore($\infty/2$) e la regione di accettazione($1-\infty$).  
La variabile aleatoria che andiamo a considerare è: $$Z_n = \frac{\overline{X}_n -\mu}{\sigma / \sqrt{n}}$$
Di seguito viene riportato lo script R per determinare l'intervallo di confidenza

```{r eval = FALSE}
#grado di confidenza
alpha <- 1 - 0.95
limite <- qnorm(1 - alpha/2, mean = 0, sd = 1)
size <- length(popolazioneAtleti$km)
print(paste("limite inf. =", round(-limite, digits = 2),
            "limite sup. =", round(limite, digits = 2))
      )
r1 <- valMedio - limite * stdDev/sqrt(size)
r2 <- valMedio + limite * stdDev/sqrt(size)
print(paste("r1 =", round(r1, digits = 2),
            "valore medio =", round(valMedio, digits = 2),
            "r2 =", round(r2, digits = 2))
      )
```

è stato assegnato un intervallo di confidenza pari a 0.95 quindi la regione di accettazione sarà di 0.95 di conseguenza la regione di rifiuto sarà di 0.25. Con la funzione __qnorm()__ sono stati calcolati i limiti inferiori e superiori della variabile aleatoria.  
Di seguito viene riportato il grafico della densità normale standard con i valori che riguardano il nostro intervallo di confidenza. Infine è stata calcolata la **stima dell'intervallo di confidenza** i valori sono stati salvati in __r1__ e __r2__, possiamo notare che l'intervallo di stima va da 22.7 a 23.84 e che il valore medio è compreso in questo intervallo.

```{r echo=FALSE, eval = FALSE}
plotDensita("Densità normale standard", 0.95, 0.05, limite)
```

## 7.3.2 Intervallo per $\mu$ con non $\sigma^2$ nota
Per questo studio verrà utilizzata la **densità di Student con n-1 gradi di libertà**. Anche in questo caso per determinare il valore medio possiamo utilizzare il metodo pivotale con la seguente variabile aleatoria: $$T_n = \frac{\overline{X}_n -\mu}{S_n / \sqrt{n}}$$.  
In questo caso il calcolo dei limiti viene fatto con la funzione __qt()__ e poi possiamo andare a calcolare la regione ammissibile, di seguito viene riportato lo script per il calcolo della stima dell'intervallo in cui ricade $\mu$

```{r eval = FALSE}
alpha <- 1 - 0.99
limite <- qt(1 - alpha/2, df = size-1)
print(paste("limite inf. =", round(-limite, digits = 2),
            "limite sup. =", round(limite, digits = 2))
      )
r1 <- valMedio - limite * stdDev/sqrt(size)
r2 <- valMedio + limite * stdDev/sqrt(size)
print(paste("r1 =", round(r1, digits = 2),
            "valore medio =", round(valMedio, digits = 2),
            "r2 =", round(r2, digits = 2))
      )
```

Avendo aumentato i margini di accettazione(da 0.95 a 0.99) la diretta conseguenza sarà il fatto che i margini di rifiuto saranno più bassi. La ragione di accattazione va da -2.62 a 2.62, con una grado di fiducia molto alto la stima dell'intervallo di confidenza è più ampia infatti va da 22.51 a 24.03 e anche in questo caso il valore medio ricade nell'intervallo.  
Di seguito viene riportato il grafico della densità normale standard con i valori che riguardano il nostro intervallo di confidenza. Infine è stata calcolata la **stima dell'intervallo di confidenza** i valori sono stati salvati in __r1__ e __r2__, possiamo notare che l'intervallo di stima va da 22.51 a 24.03 e che il valore medio è compreso in questo intervallo.

```{r echo=FALSE, eval = FALSE}
plotDensita("Densità di Student con n-1 gradi di libertà", 0.99, 0.01, limite)
```

## 7.3.3 Intervallo per $\sigma^2$ con $\mu$ nota
Per determinare l'intervallo di confidenza per $\sigma^2$ con $\mu$ nota utilizziamo un'altra volta il metodo pivotale con la seguente variabile aleatoria di pivot
$$V_n = \sum_{i=1}^{n}\left ( \frac{X_i - \mu}{\sigma} \right )^2$$
Essa dipende dal campione casuale e dal parametro non noto $\sigma^2$ ed è distribuita con la legge **$\chi^2$ con n gradi di libertà**.  
Di seguito viene mostrato lo script R per calcolare l'intervallo di confidenza
```{r eval = FALSE}
varianza <- var(popolazioneAtleti$km)
print(paste("varianza =", round(varianza, digits = 2)))
alpha <- 1 - 0.95
mu <- trunc(valMedio)
confInf <- qchisq(alpha/2, df = size)
confSup <- qchisq(1 - alpha/2, df = size)
print(paste("limite inf.=", round(confInf, digits = 2), 
            "limite sup.=", round(confSup, digits = 2)))
r1 <- ((size-1)*varianza+size*(valMedio-mu)**2) / confSup
r2 <- ((size-1)*varianza+size*(valMedio-mu)**2) / confInf
print(paste("r1=", round(r1, digits = 2), 
            "r2=", round(r2, digits = 2)))
```

Il grado di fiducia assegnato in questo caso è 0.95, in seguito è stata calcolata la densità con il metodo del **$\chi^2$ con n gradi di libertà** dove n è pari alla grandezza del campione(__size__). La densità è stata calcolata sia per il limite superiore che per quello inferiore, i risultati sono riportati nello script soprastante. Infine è stata calcolata la stima per dell'intervallo di confidenza di grado 0.95, l'intervallo va da 7.21 a 12.26 e la varianza(9.25) è compresa in tale intervallo. Di seguito viene riportato il grafico

```{r echo=FALSE, eval = FALSE}
curve(dchisq(x, df= 6), from = 0, to = 12, axes = FALSE, ylim=c(0,0.15),
      xlab = "", ylab = "", main="Densità chiquadro con n gradi di libertà")
text(4, 0.02, 0.95)
axis(1, c(0, 2, 4, 6, 12), c("", round(confInf, digits = 2),
                             expression(n-2),
                             round(confSup, digits = 2), ""))
vals <- seq(0,2, length = 100)
x<- c(0, vals, 2, 0)
y<- c(0, dchisq(vals, df=6),0,0)
polygon(x, y, density = 20, angle=45)
vals<- seq(6,12, length = 100)
x <- c(6, vals,12 ,6)
y <- c(0, dchisq(vals, df = 6), 0, 0)
polygon(x, y, density = 20, angle = 45)
abline(h=0)
text(1.2,0.02,paste(0.05, "/2", sep = ""))
text(8.5,0.02,paste(0.05, "/2", sep = ""))
```

## 7.3.4 Intervallo per $\sigma^2$ con non $\mu$ nota
Per determinare un intervallo di confidenza da 1 a $\infty$ per la varianza nel caso in cui il valore medio della popolazione non sia noto si considera la seguente variabile aleatoria di pivot.
$$Q_n = \frac{(n-1)S^2_n}{\sigma^2} = \frac{1}{\sigma^2} \sum_{i=1}^{n}(X_i - \overline{X}_n)^2$$
Questa variabile dipende dal campione e dal parametro non noto $\sigma^2$ ed è distribuita con la legge del $\chi^2$ con n-1 gradi di libertà.  
Di seguito viene mostrato lo script per calcolare il limite superiore e inferiore per questo intervallo
```{r eval = FALSE}
alpha <- 1 - 0.95
limInf <- qchisq(alpha/2, df = size-1)
limSup <- qchisq(1 - alpha/2, df = size-1)
print(paste("limite inf.=", round(limInf, digits = 2), 
            "limite sup.=", round(limSup, digits = 2)))
r1 <- (size-1)*var(popolazioneAtleti$km)/qchisq(1-alpha/2, df = size-1)
r2 <- (size-1)*var(popolazioneAtleti$km)/qchisq(alpha/2, df = size-1)
print(paste("r1=", round(r1, digits = 2), 
            "r2=", round(r2, digits = 2)))
```

Il grado di fiducia è rimasto 0.95, il calcolo del limite inferiore e superiore è stato fatto con il metodo del $\chi^2$ con n-1 gradi di libertà, il limite inferiore è 82 mentre quello superiore è 139.78.   **L'intervallo di confidenza** va da 7.21 a 12.29, la varianza è di 9.24 quindi possiamo dire che anche in questo caso la varianza è compresa nell'intervallo.  
Di seguito viene mostrato il grafico.

```{r eval = FALSE, echo=FALSE}
curve(dchisq(x, df= 6), from = 0, to = 12, axes = FALSE, ylim=c(0,0.15),
      xlab = "", ylab = "", main="Densità chiquadro con n-1 gradi di libertà")
text(4, 0.02, 0.95)
axis(1, c(0, 2, 4, 6, 12), c("", round(limInf, digits = 2),
                             expression(n-3),
                             round(limSup, digits = 2), ""))
vals <- seq(0,2, length = 100)
x<- c(0, vals, 2, 0)
y<- c(0, dchisq(vals, df=6),0,0)
polygon(x, y, density = 20, angle=45)
vals<- seq(6,12, length = 100)
x <- c(6, vals,12 ,6)
y <- c(0, dchisq(vals, df = 6), 0, 0)
polygon(x, y, density = 20, angle = 45)
abline(h=0)
text(1.2,0.02,paste(0.05, "/2", sep = ""))
text(8.5,0.02,paste(0.05, "/2", sep = ""))
```

# 8. Intervalli di fiducia approssimati
per costruire intervalli di confidenza approssimati per grandi campioni possiamo confrontare i valori medi di due diverse popolazioni, in questo studio prenderemo i valori medi di due popolazioni normali di cui sono note le varianze.

## 8.1 Caso di studio
Osservando un campione di 180 merendine prodotte da una azienda A si è riscontrato che in media le merendine scadono dono 200 giorni dalla loro messa in commercio, invece osservando un campione di 230 merendine prodotte da un'altra azienda B si è notato che la scadenza media delle merendine è di 150 giorni.  
Supponiamo di conoscere la deviazione standard $\sigma_a = 120, \sigma_b = 80$ si vuole determinare una stima dell'intervallo di confidenza $1-\infty=0.99$ per la differenza fra le due medie.  
Ricapitolando i dati a nostra di sposizione sono:  
$A_{180} = 200, B_{230}=150$  
$\sigma^2_a= 14400, \sigma^2_a=6400$  
Di seguito viene riportato lo script per calcolare la differenza dei valori medi

```{r eval = FALSE}
alpha <- 1-0.99
zAmezzi <- qnorm(1-alpha/2, mean = 0, sd = 1)
round(zAmezzi, digits = 2)
A = 180
B = 230
meanA = 200
meanB = 150
stdDevA = 120
stdDevB = 80
muA = (meanA - meanB - zAmezzi) *sqrt(stdDevA^2/A + stdDevB^2/B)
muB = (meanA - meanB + zAmezzi) *sqrt(stdDevA^2/A + stdDevB^2/B)
toPrint<- paste("Differenza fra le medie:", round(muA, digits = 2),"-", round(muB, digits = 2))
print(toPrint)
```

Analizzando i risultati ottenuti possiamo notare che $z_{a/2} = 2.57$ e che la stima per intervallo di confidenza di grado $1 - \infty$ per le differenze delle due medie $\mu_1$ e $\mu_2$ delle scadenze è pari a 492.45 - 545.94, quindi, dato che entrambi i limiti sono positivi è possibile dedurre che le merendine prodotte dall'industia A abbiano una scadenza superiore a quelle prodotte dall'industia B.

## 8.2 $\chi^2 test$
Con il $\chi^2$ test si punta a verificare che una certa popolazione descritta da una variabile aleatoria $X$ sia caratterizzata da una funzione di distribuzione $f_x(x)$ avente k parametri non noti da stimare.
Formuliamo le ipotesi:  

- $Hp_0$: $X$ ha una funzione di distribuzione $f_x(x)$ (ipotesi nulla)
- $Hp_1$: $X$ non ha una funzione di distribuzione $f_x(x)$ (ipotesi alternativa)

Il $\chi^2$ test mira a riggettare $H_0$ al fine di accettare $H_1$.  
Siamo intenzionati a verificare se il campione dell'altezza(espressa in cm) di 105 persone di venticinque anni sia adatto ad una distribuzione normale
```{r eval = FALSE, warning=FALSE}
sample <- c(rep(165,15), rep(170, 25), rep(160, 10), rep(155, 25), rep(180, 30))
n <- 105
media <- mean(sample)
print(paste("media =", round(media, digits = 2)))
stdDev <- sd(sample)
print(paste("deviazione standard =", round(stdDev, digits = 2)))
a <- numeric(4)
for(i in 1:4){
  a[i] <- qnorm(0.2*i, mean = media, sd = stdDev) #quantili
}
a
r <- 5
nint <- numeric(r)
nint[1] <- length(which(sample < a[1]))
nint[2] <- length(which((sample >= a[1]) & (sample < a[2])))
nint[3] <- length(which((sample >= a[2]) & (sample < a[3])))
nint[4] <- length(which((sample >= a[3]) & (sample < a[4])))
nint[5] <- length((which(sample >= a[4])))
nint
chiQ <- sum(((nint-n*0.2)/sqrt(n*0.2))^2)
chiQ
k <- 2
alpha <- 0.05
limInf <- qchisq(alpha/2, df = r-k-1)
limSup <- qchisq(1 - alpha/2, df = r-k-1)
print(paste("Limite inferiore =", round(limInf, digits = 2),
            "limite superiore =", round(limSup, digits = 2)))
```

L'altezza media è di 167.62cm e la deviazione standard è di 9.51.  
In seguito il campione è stato suddiviso in 5 sottoinsiemi ed è stata applicata la funzione **qnorm()** per calcolare i quantili della distribuzione normale, l'output è il seguente

- 1° sottoinsieme ha un intervallo che va da $-\infty$ a 159.61 e contiene 25 elementi
- 2° sottoinsieme ha un intervallo che va da 159.61 a 165.20 e contiene 25 elementi
- 3° sottoinsieme ha un intervallo che va da 165.20 a 170.02 e contiene 25 elementi
- 4° sottoinsieme ha un intervallo che va da 170.02 a 175.62 e contiene 0 elementi
- 5° sottoinsieme ha un intervallo che va da 175.62 a $\infty$ e contiene 30 elementi

Il $\chi^2$ è pari a 27.14, a questo punto bisogna calcolare i gradi di libertà a chi deve essere applicata la densità del $\chi^2$. Per calcolarli si applica la formula **r-k-1** dove r sono i sottoinsieme(r=5) e k sono i parametri non noti(k = 2 i parametri non noti sono media e varianza) quindi la funzione di distribuzione ha 2 gradi di libertà.  
Andiamo a calcolare il limite inferiore e superiore e notiamo che sono pari rispettivamente a 0.05 e 7.38, dato che il valore del $\chi^2$ non rientra nell'intervallo possiamo affermare che il campione analizzato non può essere accettato come una distribuzione normale quindi possiamo **rigettare l'ipotesi nulla**.

