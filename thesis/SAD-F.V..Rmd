---
title: "Statistica e Analisi dei Dati"
author: "Francesco Vicidomini"
date: "28/7/2019"
output: 
    pdf_document: 
        toc: yes
---

```{r setup, include=FALSE}
library(readxl)
library(knitr)
library(randomcoloR)
library(plyr)
library(TeachingDemos)
source("../scripts/utils.R")
source("../scripts/nonOmo.R")
source("https://raw.githubusercontent.com/talgalili/R-code-snippets/master/boxplot.with.outlier.label.r")

knitr::opts_chunk$set(echo = TRUE)
```
<!--numerazione dell'inidice-->
\setcounter{page}{1} 		
\pagenumbering{roman} 
<!--\listoftables-->
\listoffigures
\newpage
<!--numerazione della tesina -->
\setcounter{page}{1} 		
\pagenumbering{arabic} 
# 1. Introduzione
L'obiettivo del progetto è quello di analizzare e confrontare il numero di laureati(delle università pubbliche) fra le varie regioni italiane nell'anno 2016, i dati sono stati presi dal sito _[dati.istat](http://dati.istat.it/)_ in formato xlsx e sono stati importati in R utilizzando la funzione **read_excel** dopodichè le informazioni delle colonne del dataset vengono salvate nelle  seguenti variabili 
```{r}
ds <- read_excel("../dataset/dataset.xlsx", 
                 skip = 1)

```

Una volta importato il dataset essopresenta i segenti valori  
\begin{figure}[h]
\begin{center}
\includegraphics[width=350px]{../img/ds.png}
\caption{Il dataset}
\end{center}
\end{figure}

Per semplificare il lavoro i valori delle colonne vengono salvati all'interno delle variabili riportate di seguito
```{r}
#variabili
regioni <- c("Piemonte", "Valle d'Aosta", "Liguria", "Lombardia",
             "T. Alto Adige", "Veneto", "F.V.Giulia", "E. Romagna",
             "Toscana", "Umbria", "Marche", "Lazio", "Abruzzo",
             "Molise", "Compania", "Puglia", "Basilicata",
             "Calabria", "Sicilia", "Sardegna")
row.names(ds) <- regioni
triennale <- ds$`corsi di laurea di I livello`
magistrale <- ds$`corsi di laurea magistrale biennale`
unico <- ds$`corsi di laurea magistrale a ciclo unico`
vecchio <- ds$`corsi di laurea del vecchio ordinamento`
```

## 1.1 Calcolo dei tassi
I valori risultano molto altalenanti, quindi potrebbero risultare difficili da rappresentare graficamente in futuro, per ovviare a questo problema si è pensato di rappresentare i dati come tasso di laureati per regione ogni mille abitanti, i valori sono stati poi arrotondati alle sole prime tre cifre decimale, il dataset dopo la trasformazione si presenta nel seguente modo:

\begin{figure}[h]
\begin{center}
\includegraphics[width=450px]{../img/dsTassi.png}
\caption{Tasso di laureati ogni mille abitanti}
\end{center}
\end{figure}

I valori sono stati calcolati utilizzando i seguenti comandi:

```{r}
popolazione <- c(4404000, 127329, 1571000, 10010000,
                 1062860, 4915000, 1221000, 4448000,
                 3744000, 891181, 1544000, 5888000, 
                 1327000, 312027, 5851000, 4077000,
                 573694, 1971000, 5074000, 1658000)
tassoTriennale <- (triennale / popolazione) * 1000
tassoMagistrale <- (magistrale / popolazione) * 1000
tassoUnico <- (unico / popolazione) * 1000
tassoVecchio <- (vecchio / popolazione) * 1000
dsTassi <- data.frame("Tasso laureati triennali"=round(tassoTriennale, digits = 3),
                      "Tasso laureati magistrali"=round(tassoMagistrale, digits = 3),
                      "Tasso laureati a ciclo unico"=round(tassoUnico, digits = 3),
                      "Tasso laureati vecchio ordinamento"=round(tassoVecchio, digits = 3)
                     )
dsPopolazione <- data.frame("Popolazione"= popolazione)
row.names(dsPopolazione) <- regioni
row.names(dsTassi) <- regioni

```

Nel data frame **dsPopolazione** sono presenti il numero di abitanti di ogni regione italiana nell'anno 2016; nel data frame **dsTassi** sono presenti i tassi di laureati per ogni regioni italiana per ogni tipo di laurea e anche il totale dei tassi.

# 2. Grafici a barre
Per una rapida visualizzazione dei dati si è deciso di mostrare i grafici a barre dei laureati nelle varie regioni d'Italia. I grafici verranno mostrati con la funzione scritta ad hoc **showBar**
```{r}
showBar
```
essa prende in input un dei vettori dei tassi, i nomi delle regioni per definire le label della legenda, il nome del grafico e il numero di righe del dataframe, i colori delle barre sono stati definiti grazie alla funzione **distinctColorPalette** della libreria **randomcoloR**.

## 2.1 Grafico dei tassi dei laureati triennali
```{r}
rowTassi <- nrow(dsTassi)
showBar(tassoTriennale, regioni, "Tassi Triennale", rowTassi, "topleft")
```

Il grafico ci dice che la regione con il più alto tasso di laureati triennali è l'Abruzzo(4.5) mentre la regione con il tasso più basso è la Basilicata(0.88).

## 2.2 Grafico dei tassi dei laureati magistrali
```{r}
showBar(tassoMagistrale, regioni, "Tassi Magistrale", rowTassi, "topright")
```

I tassi per i laureati magistrali si abbassano, questo è normale perchè bisogna considare il fatto che la popolazione resta sempre la stessa ma il numero di laureati Magistrali è molto più basso rispetto a quello dei laureati triennali.  
Il tasso più alto viene registrato in Emilia-Romagna(2.01), l'Abruzzo che aveva il più alto tasso di laureati Triennali continua ad avere un tasso abbastanza alto pari a 1.88, il tasso più basso viene registrato dalla Valle d'Aosta con un tasso di laureati magistrali pari a 0.09.

## 2.3 Grafico dei tassi dei laureati a ciclo unico
```{r}
showBar(tassoUnico, regioni, "Tassi ciclo unico", rowTassi,"topleft")
```

Anche in questo caso i tassi si abbassano ulteriormente dato che i laureati a ciclo unico per ogni regione sono molto meno rispetto a laureati Triennali e Magistrali.  
La regione con il tasso più alto è di nuovo l'Abruzzo con 0.93 mentre la regione con il tasso più basso è la Valle d'Aosta con 0.02.

## 2.4 Grafico dei tassi dei laureati con il vecchio ordinamento
```{r}
showBar(tassoVecchio, regioni, "Tasso vecchio ordinamento", rowTassi, "topright")
```
Per i laureati registramo i tassi più bassi di tutti dato che il numero di totale di laureati con il vecchio ordinamento in Italia nel 2016 è pari a 2727. La regione con il tasso più alto resta l'Abruzzo con 0.10, il tasso più basso si registra in Umbria avente un tasso pari a 0.

# 3. Diagramma di Pareto
La legge del Principio di Pareto(Legge dell'80/20) afferma che _"la maggior parte degli effetti dipendono da un numero ristretto di cause"_. Per costruire il diagramma di Pareto occorre classificare i dati, ordinarli, disegnare un grafico a barre, aggiungere la linea cumulativa e aggiungere le informazioni di base.  
Come dati da classificare è stato scelto il totale dei tassi di laureti italiani, di seguito viene mostrata la funzione utilizzata per costruire il diagramma.
```{r eval=FALSE, echo=TRUE}
showPareto <- function(totale, main, rows){
    #ordiniamo il dataset
    ordinato <- dsTassi[order(dsTassi$Totale, decreasing = TRUE),]
    #prendiamo le regioni ordinate
    lbls <- rownames(ordinato)
    #prendiamo il totale
    ordinato <- ordinato$Totale
    freqRel <- prop.table(ordinato)
    app <- data.frame(ordinato)
    pal<- distinctColorPalette(rows)
    x<- barplot(freqRel, ylim= c(0, 1.5), main = main, col=pal, axisname = FALSE)
    axis(1, las=2, hadj= 0.6, at=x, labels=lbls, line=1, col="transparent")
    lines(x, cumsum(freqRel), type = "b", pch=16)
    text(x-0.2, cumsum(freqRel) + 0.03, paste(format(cumsum(freqRel)*100, digits = 3), "%"))
}

showPareto(totaleTassi, "Diagramma di Pareto di tutti i laureati nel 2016", rowTassi)
```
\newpage
\begin{figure}[t]
\includegraphics{../img/pareto.png}
\caption{Pareto dei laureati italiani}
\end{figure}
Dal grafico si evince che:

- l'8.56% dei laureati italiani è abruzzese
- il 75.89% dei laureati italiani è abruzzese, romagnolo, laziale, marchigiano, umbro, friulano, toscano, campano, veneto, piemontese, molisano, alto adesino, lombardo
- il 15,55% dei laureati italiani vive nelle restanti sette regioni
Possiamo dire che il principio di Pareto non è soddisfatto perchè l'80% dei laureati esce dal più del 50% delle università sparse per le varie regioni(invece che nel 20%).

# 4. Retta di regressione
La relazione fra due variabili quantitative può essere rappresentata graficamente con un diagramma di dispessione(anche detto scatterplot). Per poter costruire questo grafico abbiamo bisogno di individuare le due variabili da mettere in relazione, la coppia di variabili è formata da una variabile **dipendente**(posta sull'asse delle Y) e una **indipendente**(posta sull'asse delle X); ogni coppia di osservazioni viene rappresentata come un punto. Il risultato finale sarà un grafico contenente un insieme di punti attraverso il quale cerchiamo di individuare un trend.  
La correlazione tra le variabili quantitative si ricava tramite la **covarianza** che indica quanto le due variabili variano insieme, ovvero quanto esse siano dipendenti.  
Il **coefficiente di correlazione** ci consente di misurare la "forza" del legame fra le due variabili prese in esame.  
Le variabili prese in esame sono:

- tasso di laureati magistrali dipendente
- tasso di laureati triennali indipendente

Le due variabili presentano una **covarianza** pari a 
```{r echo=TRUE, eval=TRUE}
cov(tassoTriennale, tassoMagistrale)
```
e un **coefficiente di correlazione** pari a 
```{r echo=TRUE, eval=TRUE}
cor(tassoTriennale, tassoMagistrale)
```
Possiamo notare che la covarianza è positiva quindi possiamo dire che i valori delle due variabili sono correlati e che tendono a variare insieme, anche il coefficiente di correlazione è positivo e assume un valore molto prossimo ad 1 questo ci da una ulteriore conferma della bontà di questo legame.

## 4.1 Regressione lineare semplice
Il modello di regressione lineare semplice è esprimibile attraverso l'equazione di una retta tale da riuscire a interpolare i punti della nuvola dello scatterplot meglio di tutte le altre rette possibili.  
L'equazione della retta è:
$$Y =  \alpha + \beta X$$
Dove alpha è l'**intercetta** e beta il **coefficiente angolare**.  
L'intercetta corrisponde all'ordinata del punto di intersezione tra la retta interpolante e l'asse delle ordinate. Il coefficiente angolare indica la pendenza della retta un coefficiente positivo indica una retta crescente, mentre, un coefficiente angolare negativo indica una retta descrescente. 

## 4.2 Costruzione del modello di regressione lineare semplice
```{r echo=TRUE, eval=TRUE}
slr <- lm(tassoTriennale~tassoMagistrale)
print(slr)
```
Possiamo notare che il coefficiente angolare è positivo quindi avremo una retta crescente.  
Di seguito viene mostrato lo script fatto per mostrare lo scatterplot della retta di regressione, per una migliore visualizzazione i dati riguardanti i tassi triennali e magistrali sono stati scalati utilizzando la funzione **scale()**.
```{r echo=TRUE, eval=TRUE}
scaleTriennale <- scale(tassoTriennale)
scaleMagistrale <- scale(tassoMagistrale)
modelloScalato <- lm(scaleTriennale~scaleMagistrale)
plot(scaleTriennale, scaleMagistrale, main= "Retta di regressione", xlab="tasso Triennale",
ylab = "tasso Magistrale", col="red")
abline(modelloScalato)
stime <- fitted(modelloScalato)
segments(scaleTriennale, stime, scaleTriennale, scaleMagistrale, col="magenta")
text(x=scaleTriennale, y=scaleMagistrale, label = regioni, cex = 0.6)
```

Dal grafico si evince che il campione è positivamente correlato in quanto abbiamo una retta interpolante crescente e che avendo una covarianza positiva ma non di molto i segmenti non presentano sempre la stessa distanza.

## 4.3 Grafico dei residui
Con l'analisi dei residui effettuiamo una analisi più accurata sul come la retta interpola i dati e di come i residui si dispongono intorno alla retta interpolante influenzandone la posizione. I residui sono posti sulle ordinate mentre e i valori della variabile indipendente sono disposti sull'asse delle ascisse.  
Come prima cosa vengono calcolati i residui del modello con la funzione **resid()** dopodichè viene stampato il grafico dei residui
```{r echo=TRUE, eval=TRUE}
residui<-(resid(slr))
plot(tassoMagistrale, residui, main = "Grafico dei residui", xlab = "Tasso Magistrale",
ylab = "Residui", col= "red")
text(x= tassoMagistrale, y=residui+0.05, labels = regioni, cex=0.6)
abline(h=0, col="blue", lty=2)
```

# 5. Statistica descrcittiva

## 5.1 Quantile
La funzione **quantile()** è utile per analizzare un campione di dati, essa restituisce cinque valori:

- Q0 valore minimo dei dati
- Q1 primo quantile, questo valore corrisponde al dato per il quale il 25% dei dati risultano alla sinistra del valore e il 75% risulta a destra
- Q2 secondo quantile, questo valore corrisponde al dato per il quale il 50% dei dati risultano alla sinistra del valore e il 50% risulta a destra; questo valore viene anche chiamato **mediana**
- Q3 terzo quantile, questo valore corrisponde al dato per il quale il 75% dei dati risultano alla sinistra del valore e il 25% risulta a destra
- Q4 valore massimo dei dati

## 5.2 Calcolo dei quantili
In questo paragrafo verranno calcolati i quantili per le varie colonne del dataset dei tassi, i valori percentiali corrispondono ai seguenti quantili:

- 0% sta a Q0
- 25% sta a Q1
- 50% sta a Q2
- 75% sta a Q3
- 100% sta a Q4

```{r echo = TRUE, eval = TRUE}
quantile(tassoTriennale)
quantile(tassoMagistrale)
quantile(tassoUnico)
quantile(tassoVecchio)
```

## 5.3 Boxplot
Con il boxplot andiamo a rappresentare graficamente quanto è stato calcolato con in quantili. La rappresentazione grafica corrisponde ad una scatola i cui estremi corrispondono a Q1 e Q3, e da una linea centrale che rappresenta Q2, in alto e in basso sono presenti due linee orizzontali dette "baffi".  
Il baffo inferiore corrisponde al valore più piccolo delle osservazioni Q0
$$Q_0 >= Q_1-1.5*(Q_3-Q_1)$$
il baffo superiore corrisponde al valore più alto delle osservazioni(Q4) 
$$Q_4 <= Q_3+1.5*(Q_3-Q_1)$$
Se tutti i dati rientrano nell'intervallo
$$ Q_1-1.5*(Q_3-Q_1) <= x <= Q_3+1.5*(Q_3-Q_1)$$
allora i baffi saranno posti in corrispondenza del valore minimo e del valore massimo del campione, se invece sono presenti valori al di fuori dell'intervallo questi verranno considerati come valori anomomali(**outlier**) i valori anomali vanno comunque analizzati per capire da cosa è stato scaturito quel valore anomalo.
Il boxplot può essere utilizzato anche per esaminare altre caratteristiche della distribuzione quali:

- **centralità** espressa come mediana
- **forma** la forma che può assumere può essere simmatrica o asimmetrica essa può essere dedotta esaminando le distanza tra il primo e il terzo quartile
- **dispersione** possiamo dedurre la dispersione esaminando la distanza che passa da Q1 a Q3  

Per mostrare i boxplot è stata sviluppata la funzione **showBarPlot**
```{r, eval=FALSE}
showBoxPlot <- function(label, tasso, name, title, color, lim){
    boxplot.with.outlier.label(label_name = label, tasso,
    names=c(name), main=title,
    col= color, pars=list(ylim=c(0,lim)))
}
#esempio di chiamata della funzione
showBoxPlot(regioni, tassoTriennale, "Tasso Triennale",
            "Boxplot tasso laureati triennali", "green", 5)
showBoxPlot(regioni, tassoMagistrale, "Tasso Magistrale",
            "Boxplot tasso laureati magistrali", "yellow", 3)
showBoxPlot(regioni, tassoUnico, "Tasso Ciclo Unico",
            "Boxplot tasso laureati ciclo unico", "blue", 2)
showBoxPlot(regioni, tassoVecchio, "Tasso Vecchio Ordinamento",
            "Boxplot tasso laureati vecchio ordinamento", "purple", 0.3)
```

La funzione showBar per disegnare i boxplot con le label usa la funziona **boxplot.with.outlier.label** presa da __\href{https://raw.githubusercontent.com/talgalili/R-code-snippets/master/boxplot.with.outlier.label.r}{questo repository}__.  
Showbar prende in input:

* le label che sarebbero le regioni,
* la colonna dei tassi,
* il nome della colonna,
* titolo da dare al boxplot,
* colore del box,
* lunghezza dell'asse y


\begin{figure}[h]
\begin{center}
\includegraphics{../img/boxplot.png}
\caption{Boxplot dei laureati}
\end{center}
\end{figure}

## 5.4 Analisi del boxplot
Analizzando il boxplot possiamo arrivare alle seguenti conclusioni:

* l'unico outlier è presente nella colonna dei tassi triennali ed è la regione Abruzzo con 4.5
* i dati risultano avare una maggiore dispersione nella colonna dei laureati triennali
* possiamo notare la compattezza dei dati relativi alle colonne del tasso dei laureati a ciclo unico e del tasso dei laureati con vecchio ordinamento
* per i laureati magistrali la maggior parte dei dati si trova alla sinistra della mediana

# 6. Analisi dei cluster
L'analisi dei cluster è una metodologia che permette di raggruppare un campio ampio vari sottoinsiemi più piccoli detti cluster. L'obettivo di questa analisi è quello di **raggruppare** fra loro elementi simili.

## 6.1 Costruizione 
Abbiamo detto che raggruppiamo in base alla **somiglianza** degli elementi, per calcolare questa somiglianza applichiamo i metodi delle distanze. Le distanze tra tutte le possibili coppie di unità vengono inserite in una matrice simmetrica _D_ di cardinalità _n x n_. Il metodo che che utilizzero per calcolare le distanze è quello della **metrica Euclidea**.  
Avendo definito come misurare la distanza andiamo a definire come raggruppare le unità osservate, esistono tre metodi per il raggruppamento:

1. Eunumerazione completa
2. Metodi gerarchici
3. Metodi non gerarchici

L'**enumerazione completa** è molto onerosa dal punto di vista computazionale, in quanto questo metodo cerca di ottimizzare(minimizzare) una fissata funzione obiettivo per ogni possibile partizione delgi individui all'interno di un cluster.  
I **metodi gerarchici** possono essere di due tipi **agglomerativi** o **divisivi**, i tipi agglomerativi partono da una situazione in cui ci sono _n_ cluster distinti con cardinalità pari a uno, per arrivare a un solo cluster con cardinalità _n_. Invece i metodi divisi fanno l'inverso dei metodi agglomerativi, ovvero, partono da un unico cluster con cardinalità _n_ per arrivare ad _n_ cluster con cardinalità 1.  
L'obiettivo dei metodi gerarchi è arrivare alla costruzione di un **dendrogramma** ovvero in grafico ad albero che rappresenta la sequenza delle partizioni. Il dendrogramma riporta sull'ordinata i livelli di distanza e sulle ascisse i signoli elementi, questa rappresentazione consente di farci capire rapidamente i livelli di distanza fra gli elementi.  
I **metodi non gerarchici** permettono di riallocare gli elementi già classificati in un precedente livello di analisi.

## 6.2 Calcolo delle misure di non omogeneità interna e totale
Le misure di non omogeneità ci servono per calcolare quanti gli elementi siano distinti fra loro, esse vanno calcolate sia all'interno del sigolo cluster sia tra due cluster distinti. Una volta calcolate le misure saremo in grado di dire che gli elementi appartenti allo stesso cluster sono quanto più possibile omogenei fra loro a differenza di due(o più) elementi appartenti a due(o più) cluster distinti.  
Gli individui vanno scelti in modo tale da minimizzare le misure di non omogeneità all'interno dei cluster(within) e da massimizzare la misura di non omogeneità tra gruppi distinti(between).  
Questo calcolo va effettuato sia per i metodi gerarchici sia per i metodi non gerarchici e corrisponde al _rapporto fra la somma dei quadrati degli elementi sotto la diagonale principale della matrice delle distanze e il numero degli individui_

## 6.3 Cluster con metodi non gerarchici: k-means
Il metodo non gerachico k-means richiede che il numero di cluster venga specificato a priori e da in output un unica partizione. Per garantire la convergenza del metodo iterativo si considera la matrice contenete i quadrati delle distanze euclidee.  
Vantaggi del k-means:  

* rapidità nei calcoli
* gli elementi hanno la possibilità di raggrupparsi o allontanarsi

Fra gli svantaggi c'è il fatto che la classificazione finale può essere influenzata da:

* ordine in cui sono stati presi i vettori
* proprietà geometriche dei vettori delle misure
* scelta iniziale dei _k_ vettori
* possibile convergenza versono un **ottimo locale** ciò implica che considerando un insieme di partenza diverso si più giungere a una diversa partizione finale.

### 6.3.1 Implementazione k-means
Il metodo k-means è stato implementato con la funzione **mykm** che vado a mostrare di seguito
```{r, echo=FALSE, eval=TRUE}
print(mykm)
```

La funzione prende in input un dataframe(**ds**), il numero di cluster(**k**) e il numero di iterazioni da svolgere(**iter**) sceglie i centroidi come punto di riferimento e rappresenta graficamente i cluster.
Il dataframe dato in input corrisponde alle prime due colonne del dataset dei tassi dei laureati quindi al tasso di laureati triennali e magistrali, il numero di cluster va da 2 a 5, e le iterazioni sono sempre 10.

```{r, echo=TRUE, eval=TRUE}
mykm(dsTassi[1:2], 2, 10)

mykm(dsTassi[1:2], 3, 10)

mykm(dsTassi[1:2], 4, 10)

mykm(dsTassi[1:2], 5, 10)
```

## 6.4 Osservazioni finali
I valori di non omogeneità migliori si ottengo nella partizionamento con k = 4 e k = 5 infatti i loro valori sono rispettivamente 87.0% e 90.9%, si è decisio di fermarsi con la divisione in cluster con un k = 5 in quanto già l'output del k-means con 5 cluster ci mostara un alto grado di non omogeneità pari al 90.9% ma possiamo notare(sia con k=4 che con k=5) che ci sono dei cluster molto piccoli composti da uno e due elementi e quindi l'utilità del cluster si va a perdere. Quindi si è deciso di prendere in esame le partizioni con 2 e 3 cluster che hanno come valore di non omogeneità rispettivamente 55.6% e 73.1% le misure ottenute da questi due tipi di partizionamento verranno poi confrontate con le misure ottenute dai metodi gerarchici.

## 6.5 Metodi gerarchici
I metodi gerarchici sono di cinque tipi di:

* singolo
* completo
* medio
* centroide
* mediana

I metodi del centroide e della mediana non calcolano la matrice della distanza partendo dalle distanze precedenti ma dai baricentri dei cluster.
L'applicazione dei metodi gerarchici verrà svolta applicando la distanza euclidea, ogni metodo gerarchico viene calcolato dividendo i dati prima in due e poi in tre cluster in modo tale da poter confrontare i rusiltati ottenuti con il risultato del metodo non gerarchico k-means.

## 6.5.1 Metodo del legame singolo
Con questo metodo la distanza fra il gruppo $G_1$(contenente $n_1$ individui) e il gruppo $G_2$(contenente $n_2$ individui) viene definita come la minima distanza fra tutte le $n_1n_2$ distanze calcolabili fra ogni individuo di $G_1$ e ogni individuo di $G_2$.  
Inizialmente consideriamo un insieme di cluster, in seguito si cerca nella matrice delle distanze il coefficiente di distanza minima e si raggruppano gli individui nello stasso cluster.  
Se due individui riusultano simili verrà costruito un cluster con questi due elementi se gli elementi simili sono più di due verrà effettuata una scelta casuale e si procederà alla creazione di una nuova matrice. Questo processo verrà ripetuto fin quando sarà possibile aggiungere individui al cluster.  
**Vantaggi**: risulta più facile individuare gruppi di qualsiasi forma ed è più facile mettere in evidenza valori anomali
**Svantaggi**: si possono trovare nello stesso cluster individui non simili

```{r echo=FALSE}
cluster2Table <- buildClusterTable(2)
cluster2Table$B.T[1] <- "55.6%"
cluster3Table <- buildClusterTable(3)
cluster3Table$B.T[1] <- "73.1%"

#metodo del legame singolo 2 cluster
btwTot <- metodiGerarchici(scale(dsTassi[1:2]), regioni, 2, "single", "Metodo del legame singolo")
cluster2Table$B.T[2] <- btwTot
print(paste("BetweenTotal =",btwTot))

#metodo del legame singolo 3 cluster
btwTot <- metodiGerarchici(scale(dsTassi[1:2]), regioni, 3, "single", "Metodo del legame singolo")
cluster3Table$B.T[2] <- btwTot
print(paste("BetweenTotal =",btwTot))
```

## 6.5.2 Metodo del legame completo

```{r echo=FALSE}
#metodo del legame completo 2 cluster
btwTot <- metodiGerarchici(scale(dsTassi[1:2]), regioni, 2, "complete", "Metodo del legame completo")
cluster2Table$B.T[3] <- btwTot
print(paste("BetweenTotal =",btwTot))

#metodo del legame completo 3 cluster
btwTot <- metodiGerarchici(scale(dsTassi[1:2]), regioni, 3, "complete", "Metodo del legame completo")
cluster3Table$B.T[3] <- btwTot
print(paste("BetweenTotal =",btwTot))
```

## 6.5.3 Metodo del legame medio

```{r echo=FALSE}
#metodo del legame medio 2 cluster
btwTot <- metodiGerarchici(scale(dsTassi[1:2]), regioni, 2, "average", "Metodo del legame medio")
cluster2Table$B.T[4] <- btwTot
print(paste("BetweenTotal =",btwTot))

#metodo del legame medio 3 cluster
btwTot <- metodiGerarchici(scale(dsTassi[1:2]), regioni, 3, "average", "Metodo del legame medio")
cluster3Table$B.T[4] <- btwTot
print(paste("BetweenTotal =",btwTot))
```

## 6.5.4 Metodo del centroide

```{r echo=FALSE}
#metodo del centroide 2 cluster
btwTot <- metodiGerarchici(scale(dsTassi[1:2]), regioni, 2, "centroid", "Metodo del centroide")
cluster2Table$B.T[5] <- btwTot
print(paste("BetweenTotal =",btwTot))

#metodo del centroide 3 cluster
btwTot <- metodiGerarchici(scale(dsTassi[1:2]), regioni, 3, "centroid", "Metodo del centroide")
cluster3Table$B.T[5] <- btwTot
print(paste("BetweenTotal =",btwTot))
```

## 6.5.4 Metodo della mediana

```{r echo=FALSE}
#metodo del centroide 2 cluster
btwTot <- metodiGerarchici(scale(dsTassi[1:2]), regioni, 2, "median", "Metodo della mediana")
cluster2Table$B.T[6] <- btwTot
print(paste("BetweenTotal =", btwTot))

#metodo del centroide 3 cluster
btwTot <- metodiGerarchici(scale(dsTassi[1:2]), regioni, 3, "median", "Metodo del centroide")
cluster3Table$B.T[6] <- btwTot
print(paste("BetweenTotal =", btwTot))
```

## 6.6 Tabelle riassuntive

```{r echo=FALSE}
kable(cluster2Table)
kable(cluster3Table)
```

# 7. Inferenza statistica
Una indagine statisica si effettua su una popolazione che può essere finita o infinita, le caratteristiche della popolazione possono essere ottenute osservando la totalità della popolazione oppure un sottoinsieme di essa(un campione) se ci troviamo di fronte a una popolazione infita possiamo studiare le sue caratteristiche solamente attraverso un suo campione.  
L'inferenza statistica ha il compito di estendere le caratteristiche osservate in un un campione a tutta la popolazione, il problema dell'inferenza statistica è quello di riuscire a studiare la popolazione attraverso un parametro non noto, l'inferenza statistica si basa su due metodi di indagine essi sono **stima dei parametri** e **verifica delle ipotesi**.  
La stima dei parametri ha lo scopo di determinare i valori non noti delle caratteristiche di una popolazione come ad esempio media, mediana, varianza ecc... dai valori delle caratteristiche ricavati possiamo effettuare due tipi di stime, la **stima puntuale** dove stimiamo un paramentro non noto usando un singolo valore reale e la **stima per intervallo** dove si cerca di individuare un limite inferiore e un limite superiore(un intervallo) entro i quali sia compreso il parametro non noto con un certo **grado di fiducia**.  
Per quanto riguarda la verifica delle ipotesi si definisce una ipotesi se un parametro non noto del campione è accettabile.  
Per studiare i problemi dell'inferenza statistica occorre conoscere le variabili aleatorie **discrete**(assume un numero finito di valori) o **continue**(può assumere infiniti valori nell'intevallo preso in considerazione). La variabile scelta per questo studio è la variabile aleatoria normale, essa è una variabile continua.

## 7.1 Variabile aleatoria normale
La normale costituisce una distribzione limite alla quale tendono le variabili aleatoria in opportuni casi.  
Prenderemo in esame la tematica "**chilometri di corsa percorsi da 110 atleti durante i loro allenamenti settimanali**"\footnote{per motivi di spazio il data frame popolazioneAtleti non viene mostrato nella tesina ma può essere visto lanciando il comando \textit{View(popolazioneAtleti)}} Il campione di atleti è stato creato generato usando la funzione __rnorm()__. 
```{r echo = TRUE, eval = FALSE }
popolazioneAtleti <- data.frame(km = floor(rnorm(110, 24, 3)))
#salvo i dati in un CSV
write.table(popolazioneAtleti, file = "dataset/sample.csv", row.names = FALSE, sep = ";")
```

Per calcolare la  densità di una variabile aleatoria $X$ è data dalla seguente formula:
$$f_X(x) = \frac{1}{\sigma \sqrt{2\pi }} exp\left \{ - \frac{(x - \mu)^2)}{2 \sigma^2} \right \}, x \epsilon \mathbb{R} (\mu \epsilon \mathbb{R}, \sigma > 0)$$
la densità di __popolazioneAtleti__ in viene calcolata dal seguente script R
```{r echo=FALSE, eval=TRUE}
popolazioneAtleti <- read.csv(file="../dataset/sample.csv", header=TRUE, sep=";")
```

```{r}
valMedio <- mean(popolazioneAtleti$km)
stdDev <- sd(popolazioneAtleti$km)
print(paste("Valore medio = ", round(valMedio, digits=2),
            ", Deviazione standard =", round(stdDev, digits = 2))
      )

curve(dnorm(x, mean = valMedio, sd = stdDev), 
      from = 13, to = 35,
      main = "Km percorsi dai 110 atleti in una settimana di allenamento"
      )
abline(v=valMedio, col= "red", lwd=2, lty = 2)
```

Per calcolare la **densità** come prima cosa calcoliamo il valore medio e deviazione standard del campione, infine con la funzione __curve()__ che prende come input la densità normale del campione calcolata con la funzione __dnorm(valore medio, deviazione standard)__ e i parametri __from__ e __to__ che indicano l'intervallo del grafico, sul grafico è stata anche riportata la retta che rappresenta il valore medio, si può notare che il valore medio(23.27) coincide con massimo della curva, essendoci una deviazione standar alta(3.04) possiamo dire che al decrescere della deviazione standard la curva si appiattirà.  
Per calcolare la **funzione di distribuzione normale** si usa la funzione __pnorm()__ di R, di seguito viene mostrata il codice per calcolare la funzione di distribuzione normale sul campione.
```{r}
titolo = paste("km fatti in una settimana di allenamento mu =",
               round(valMedio, digits = 2),
               ", sigma =", round(stdDev, digits = 2)
               )
curve(pnorm(x, mean = valMedio, sd = stdDev), from = 13, to = 35, main = titolo)
abline(v=20, col= "red", lwd= 1, lty = 2)
abline(v=28, col= "red", lwd= 1, lty = 2)
```
Notiamo che la funzione cresce nell'intervallo che va da 20 a 28 per poi arrivare ad 1 intorno al 29 per poi stabilizzarsi.  
Possiamo anche calcolare i quantili del campione preso in analisi grazie alla funzione R __qnorm()__ 
```{r}
q <- c(0, 0.25, 0.50, 0.75, 1)
qnorm(q, mean = valMedio, sd = stdDev)
```
Conclusioni: 

* **25%** dei km è pari a 21.22 vicino al punto in cui la probabilità comincia a crescere
* **50%** dei km è pari a 23.27 uguale al valore medio
* **75%** dei km è pari a 25.32 

## 7.2 Stima dei parametri non noti
Possiamo stimare i parametri non noti con due metodi, **metodo dei momenti** **metodo della massima verosomiglianza**. Dato che vogliamo stimare i paramentri non noti di una distribuzione normale la scelta
del metodo stimatore è indifferente perchè entrambi i metodi ci porteranno allo stesso risultato.  
Troviamo uno stimatore per il valore medio(media campionaria)$\mu$ e uno per la varianza $\sigma^2$
```{r}
mu <- valMedio
sigmaQ <- (length(popolazioneAtleti$km)-1) * var(popolazioneAtleti$km)/length(popolazioneAtleti$km)
```

```{r echo=FALSE}
print(paste("stimatore valore medio mu =", mu, ", stimatore varianza sigma^2 =", sigmaQ))
```

## 7.3 Intervalli di confidenza
Alla stima puntuale di un parametro non noto di un campione si preferisce un **intervallo di confidenza** entro il quale sarà presente in parametro non noto con un certo **grado di fiducia**.  
In una distribuzione normale si cerca di individuare un intervallo di confidenza da 1 a $\infty$ per:

1. $\mu$ nel caso in cui $\sigma^2$ della popolazione sia nota
2. $\mu$ nel caso in cui $\sigma^2$ della popolazione **non** sia nota
3. $\sigma^2$ nel caso in cui $\mu$ della popolazione sia nota
4. $\sigma^2$ nel caso in cui $\mu$ della popolazione **non** sia nota

Tutti i casi elencati in precedenza sono riassunti nel seguente grafico:

```{r echo=FALSE}
curve(dnorm(x,mean=0,sd=1),from=-3, to=3,axes=FALSE,ylim=c(0,0.5), xlab="",ylab="",main="Densità normale standard")
text(0,0.05,expression(1-alpha))
axis(1,c(-3,-1,0,1,3),c("",expression(-z[alpha/2]), 0,expression(z[alpha/2]),""))
vals<-seq(-3,-1,length=100)
x<-c(-3,vals,-1,-3)
y<-c(0,dnorm(vals),0,0)
polygon(x,y,density=20,angle=45)
vals<-seq(1,3,length=100)
x<-c(1,vals,3,1)
y<-c(0,dnorm(vals),0,0)
polygon(x,y,density=20,angle=45)
abline(h=0)
text(-1.5,0.05,expression(alpha/2))
text(-2, 0.20, "Regione rifiuto")
text(1.5,0.05,expression(alpha/2))
text(2, 0.20, "Regione rifiuto")
text(0, 0.30, "Regione accettazione")
```

## 7.3.1 Intervallo per $\mu$ con $\sigma^2$ nota
Utilizziamo il **metodo pivotale** e consideriamo una variabile aleatoria con valore medio nullo e varianza unitaria. La variabile aleatoria standardizzata dipende dal campione scelto e dal parametro non noto $\mu$ per questo può essere interpretata come una variabile aleatoria di pivot. Applicando il metodo pivotale conosciamo il limite inferiore($\infty/2$) e la regione di accettazione($1-\infty$)
```{r}
#grado di confidenza
alpha <- 1 - 0.95
limite <- qnorm(1 - alpha/2, mean = 0, sd = 1)
size <- length(popolazioneAtleti$km)
r1 <- valMedio - limite * stdDev/sqrt(size)
r2 <- valMedio + limite * stdDev/sqrt(size)
print(paste("limite inf. =", round(-limite, digits = 2),
            "limite sup. =", round(limite, digits = 2))
      )
print(paste("r1 =", round(r1, digits = 2),
            "valore medio =", round(valMedio, digits = 2),
            "r2 =", round(r2, digits = 2))
      )
```

è stato assegnato un intervallo di confidenza pari a 0.95 quindi la regione di accettazione sarà di 0.95 di conseguenza la regione di rifiuto sarà di 0.25. Con la funzione __qnorm()__ sono stati calcolati i limiti inferiori e superiori della variabile aleatoria.  
Di seguito viene riportato il grafico della deviazione standard con i valori che riguardano il nostro intervallo di confidenza. Infine è stata calcolata la **stima dell'intervallo di confidenza** i valori sono stati salvati in __r1__ e __r2__, possiamo notare che l'intervallo di stima va da 22.7 a 23.84 e che il valore medio è compreso in questo intervallo.
```{r echo=FALSE}
curve(dnorm(x,mean=0,sd=1),from=-3, to=3,axes=FALSE,ylim=c(0,0.5), xlab="",ylab="",main="Densità normale standard")
text(0,0.05, 0.95)
axis(1,c(-3,-1,0,1,3),c("", round(-limite, digits = 2), 0, round(limite, digits = 2),"")) 
vals<-seq(-3,-1,length=100)
x<-c(-3,vals,-1,-3)
y<-c(0,dnorm(vals),0,0)
polygon(x,y,density=20,angle=45)
vals<-seq(1,3,length=100)
x<-c(1,vals,3,1)
y<-c(0,dnorm(vals),0,0)
polygon(x,y,density=20,angle=45)
text(-1.5,0.05,expression(0.05/2))
text(-2, 0.20, "Regione rifiuto")
text(1.5,0.05,expression(0.05/2))
text(2, 0.20, "Regione rifiuto")
text(0, 0.30, "Regione accettazione")
```

## 7.3.2 Intervallo per $\mu$ con non $\sigma^2$ nota
## 7.3.3 Intervallo per $\sigma^2$ con $\mu$ nota
## 7.3.4 Intervallo per $\sigma^2$ con non $\mu$ nota